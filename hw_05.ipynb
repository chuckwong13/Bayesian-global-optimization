{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "Covers Lectures 16, 17, 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1: Calibrating the reaction rates kappa [0-4] in Catalytic Conversion of Nitrate to Nitrogen\n",
    "\n",
    "This is Example 3.1 of [(Tsilifis, 2014)](http://arxiv.org/abs/1410.5522).\n",
    "\n",
    "Consider the catalytic\n",
    "conversion of nitrate ($\\mbox{NO}_3^-$) to nitrogen ($\\mbox{N}_2$) and other\n",
    "by-products by electrochemical means.\n",
    "The mechanism that is followed is complex and not well understood.\n",
    "The experiment of \\cite{katsounaros} confirmed the\n",
    "production of nitrogen ($\\mbox{N}_2$), ammonia\n",
    "($\\mbox{NH}_3$), and nitrous oxide ($\\mbox{N}_2\\mbox{O}$) as final products\n",
    "of the reaction, as well as the intermediate production of nitrite ($\\mbox{NO}_2^-$).\n",
    "The data are reproduced in [Comma-separated values](https://en.wikipedia.org/wiki/Comma-separated_values) (CSV) and stored in\n",
    "[data/catalysis.csv](data/catalysis.csv).\n",
    "The time is measured in minutes and the conentrations are measured in $\\mbox{mmol}\\cdot\\mbox{L}^{-1}$.\n",
    "Let's load the data into this notebook using the [Pandas](http://pandas.pydata.org) Python module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO3</th>\n",
       "      <th>NO2</th>\n",
       "      <th>N2</th>\n",
       "      <th>NH3</th>\n",
       "      <th>N2O</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>250.95</td>\n",
       "      <td>107.32</td>\n",
       "      <td>18.51</td>\n",
       "      <td>3.33</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>123.66</td>\n",
       "      <td>132.33</td>\n",
       "      <td>74.85</td>\n",
       "      <td>7.34</td>\n",
       "      <td>20.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>84.47</td>\n",
       "      <td>98.81</td>\n",
       "      <td>166.19</td>\n",
       "      <td>13.14</td>\n",
       "      <td>42.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>30.24</td>\n",
       "      <td>38.74</td>\n",
       "      <td>249.78</td>\n",
       "      <td>19.54</td>\n",
       "      <td>55.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>27.94</td>\n",
       "      <td>10.42</td>\n",
       "      <td>292.32</td>\n",
       "      <td>24.07</td>\n",
       "      <td>60.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>13.54</td>\n",
       "      <td>6.11</td>\n",
       "      <td>309.50</td>\n",
       "      <td>27.26</td>\n",
       "      <td>62.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         NO3     NO2      N2    NH3    N2O\n",
       "Time                                      \n",
       "0     500.00    0.00    0.00   0.00   0.00\n",
       "30    250.95  107.32   18.51   3.33   4.98\n",
       "60    123.66  132.33   74.85   7.34  20.14\n",
       "90     84.47   98.81  166.19  13.14  42.10\n",
       "120    30.24   38.74  249.78  19.54  55.98\n",
       "150    27.94   10.42  292.32  24.07  60.65\n",
       "180    13.54    6.11  309.50  27.26  62.54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "catalysis_data = pd.read_csv('catalysis.csv', index_col=0)\n",
    "catalysis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory of catalytic reactions guarantees that the total mass must be conserved.\n",
    "However, this is not the case in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time\n",
       "0      500.00\n",
       "30     385.09\n",
       "60     358.32\n",
       "90     404.71\n",
       "120    394.28\n",
       "150    415.40\n",
       "180    418.95\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalysis_data.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inconsistency suggests the existence of an intermediate unobserved reaction product X.\n",
    "[(Katsounaros, 2012)](http://www.sciencedirect.com/science/article/pii/S0013468612005208) suggested that the following reaction path shown in the following figure.\n",
    "\n",
    "![](../lectures/figures/scheme.png \"Reaction Scheme\")\n",
    "\n",
    "The dynamical system associated with the reaction is:\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\frac{d \\left[\\mbox{NO}_3^-\\right]}{dt} &= -k_1\\left[\\mbox{NO}_3^-\\right], \\\\\n",
    "\\frac{d\\left[\\mbox{NO}_2^-\\right]}{dt} &= k_1\\left[\\mbox{NH}_3^-\\right] - (k_2 + k_4 +\n",
    "k_5)[\\mbox{NO}_2^-], \\\\\n",
    "\\frac{d \\left[\\mbox{X}\\right]}{dt} &= k_2 \\left[\\mbox{NO}_2^-\\right] - k_3 [X],\\\\\n",
    "\\frac{d \\left[\\mbox{N}_2\\right]}{dt} &= k_3 \\left[\\mbox{X}\\right], \\\\\n",
    "\\frac{d \\left[\\mbox{NH}_3\\right]}{dt} &= k_4 \\left[\\mbox{NO}_2^-\\right],\\\\\n",
    "\\frac{d \\left[\\mbox{N}_2O\\right]}{dt} &= k_5 \\left[\\mbox{NO}_2^-\\right],\n",
    "\\end{array}\n",
    "$$\n",
    "where $[\\cdot]$ denotes the concentration of a quantity, and\n",
    "$k_i > 0$, $i=1,...5$ are the *kinetic rate constants*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions - Calibrate the reaction rates $\\kappa=(\\kappa_1,\\kappa_2,\\kappa_3,\\kappa_4,\\kappa_5)$ in the Catalysis Model to the Experimental Data\n",
    "\n",
    "Assume that you are a chemical engineer and that you are assigned the task of designing a reactor for the conversion of nitrate to nitrogen. Before you start designing, you are asked to calibrate the reaction rates $\\kappa_1,\\kappa_2,\\kappa_3,\\kappa_4,\\kappa_5$ using the observed concentration $y=(y_1,...,y_5)$ at $t=180$ stored in [data/catalysis.csv](data/catalysis.csv).\n",
    "\n",
    "We wish to calibrate the reaction rate parameters of the model to the data. Because the parameters are two small, it is desired to work with the transformed version:\n",
    "\n",
    "$$\n",
    "\\xi_i = \\log\\left(\\frac{k_i}{180}\\right).\n",
    "$$\n",
    "\n",
    "You are requested to address this problem by using the following two appeoaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Approach 1) Use Tikhonov regularization method to inverse the reaction rates $\\kappa=(\\kappa_1,\\kappa_2,\\kappa_3,\\kappa_4,\\kappa_5)$.\n",
    "\n",
    "Hint: You can use the package: https://pypi.python.org/pypi/InverseProblem/1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2) Use Bayesian inversion method to inverse the reaction rates $\\kappa=(\\kappa_1,\\kappa_2,\\kappa_3,\\kappa_4,\\kappa_5)$.\n",
    "\n",
    "We assume that the observed consentration value $y=(y_1,...,y_5)$ is contaminated with (additive) noise $\\epsilon_y$ with unknown scale $\\sigma_y^2$ in log scale; i.e.\n",
    "\n",
    "$$ \\log(y) = \\log(u(\\xi)) + \\epsilon, \\  \\epsilon \\sim \\text{N}(0,\\sigma_y^2), $$ \n",
    "\n",
    "where $u(\\xi)$ is the free of errors consentration function with respect to the reaction rates $\\kappa=(\\kappa_1,\\kappa_2,\\kappa_3,\\kappa_4,\\kappa_5)$, but free of errors.\n",
    "\n",
    "For a given value of $\\xi=(\\xi_1, \\xi_2, \\xi_3 \\xi_4, \\xi_5)$, the consentration function $u(\\xi)$ can be computed by running the Computer Model described; see Section `Computer Model' for details.\n",
    "\n",
    "Therefore, \n",
    "\n",
    "##### Statistical model   $\\mathcal{L}(y|\\xi,\\sigma_y^2)$\n",
    "\n",
    "The likelihood function is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y|\\xi,\\sigma_y^2) = \\prod_{i=1}^{n=5}\\text{LN}(y_i|\\log(u(\\xi_i)),\\sigma_y^2)\\ \n",
    "$$\n",
    "\n",
    "where $\\text{LN}(\\mu,\\sigma^2)$ denotes the the Log-Normal distribution with location paremeter $\\mu$, and scale parameter $\\sigma^2$. \n",
    "\n",
    "(Note: $X\\sim \\text{LN}(\\mu,\\sigma^2)$ iff $\\log(X)\\sim \\text{N}(\\mu,\\sigma^2)$).\n",
    " \n",
    "\n",
    "##### Your tasks:\n",
    "\n",
    "I want to calibrate the computer model against the available data\n",
    "\n",
    "1. Write a function for the log likelihood, which is the function you will maximise\n",
    "\n",
    "2. Use BGO to maximise the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computater Model\n",
    "\n",
    "This section describes how to compute the function $u(\\xi)$, where $\\xi=(\\xi_1,...,\\xi_5)$, $\\xi_i=\\log(\\kappa_i/180)$ for $i=1,...,5$, which is required for the computation of the likelihood function.\n",
    "\n",
    "We will develop a generic computational model for the solution of dynamical systems and we will use it to study the catalysis problem. The code relies on the [Fourth-order Runge-Kutta method](https://en.wikipedia.org/wiki/Runge–Kutta_methods) and is a modified copy of [http://www.math-cs.gordon.edu/courses/ma342/python/diffeq.py](http://www.math-cs.gordon.edu/courses/ma342/python/diffeq.py) developed by Jonathan Senning. The code solves:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\dot{\\mathbf{y}} &=& f(\\mathbf{y}, t),\\\\\n",
    "\\mathbf{y}(0) &=& \\mathbf{y}_0.\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def rk45( f, y0, t, args=() ):\n",
    "    \"\"\"Fourth-order Runge-Kutta method with error estimate.\n",
    "\n",
    "    USAGE:\n",
    "        y = rk45(f, x0, t, args=())\n",
    "\n",
    "    INPUT:\n",
    "        f     - function of x and t equal to dx/dt.  x may be multivalued,\n",
    "                in which case it should a list or a NumPy array.  In this\n",
    "                case f must return a NumPy array with the same dimension\n",
    "                as x.\n",
    "        y0    - the initial condition(s).  Specifies the value of x when\n",
    "                t = t[0].  Can be either a scalar or a list or NumPy array\n",
    "                if a system of equations is being solved.\n",
    "        t     - list or NumPy array of t values to compute solution at.\n",
    "                t[0] is the the initial condition point, and the difference\n",
    "                h=t[i+1]-t[i] determines the step size h.\n",
    "        args  - any other parameters of the function f.\n",
    "\n",
    "    OUTPUT:\n",
    "        y     - NumPy array containing solution values corresponding to each\n",
    "                entry in t array.  If a system is being solved, x will be\n",
    "                an array of arrays.\n",
    "\n",
    "    NOTES:\n",
    "        This version is based on the algorithm presented in \"Numerical\n",
    "        Mathematics and Computing\" 6th Edition, by Cheney and Kincaid,\n",
    "        Brooks-Cole, 2008.\n",
    "    \"\"\"\n",
    "\n",
    "    # Coefficients used to compute the independent variable argument of f\n",
    "\n",
    "    c20  =   2.500000000000000e-01  #  1/4\n",
    "    c30  =   3.750000000000000e-01  #  3/8\n",
    "    c40  =   9.230769230769231e-01  #  12/13\n",
    "    c50  =   1.000000000000000e+00  #  1\n",
    "    c60  =   5.000000000000000e-01  #  1/2\n",
    "\n",
    "    # Coefficients used to compute the dependent variable argument of f\n",
    "\n",
    "    c21 =   2.500000000000000e-01  #  1/4\n",
    "    c31 =   9.375000000000000e-02  #  3/32\n",
    "    c32 =   2.812500000000000e-01  #  9/32\n",
    "    c41 =   8.793809740555303e-01  #  1932/2197\n",
    "    c42 =  -3.277196176604461e+00  # -7200/2197\n",
    "    c43 =   3.320892125625853e+00  #  7296/2197\n",
    "    c51 =   2.032407407407407e+00  #  439/216\n",
    "    c52 =  -8.000000000000000e+00  # -8\n",
    "    c53 =   7.173489278752436e+00  #  3680/513\n",
    "    c54 =  -2.058966861598441e-01  # -845/4104\n",
    "    c61 =  -2.962962962962963e-01  # -8/27\n",
    "    c62 =   2.000000000000000e+00  #  2\n",
    "    c63 =  -1.381676413255361e+00  # -3544/2565\n",
    "    c64 =   4.529727095516569e-01  #  1859/4104\n",
    "    c65 =  -2.750000000000000e-01  # -11/40\n",
    "\n",
    "    # Coefficients used to compute 4th order RK estimate\n",
    "\n",
    "    a1  =   1.157407407407407e-01  #  25/216\n",
    "    a2  =   0.000000000000000e-00  #  0\n",
    "    a3  =   5.489278752436647e-01  #  1408/2565\n",
    "    a4  =   5.353313840155945e-01  #  2197/4104\n",
    "    a5  =  -2.000000000000000e-01  # -1/5\n",
    "\n",
    "    b1  =   1.185185185185185e-01  #  16.0/135.0\n",
    "    b2  =   0.000000000000000e-00  #  0\n",
    "    b3  =   5.189863547758284e-01  #  6656.0/12825.0\n",
    "    b4  =   5.061314903420167e-01  #  28561.0/56430.0\n",
    "    b5  =  -1.800000000000000e-01  # -9.0/50.0\n",
    "    b6  =   3.636363636363636e-02  #  2.0/55.0\n",
    "\n",
    "    n = len( t )\n",
    "    y = np.array( [ y0 ] * n )\n",
    "    for i in range( n - 1 ):\n",
    "        h = t[i+1] - t[i]\n",
    "        k1 = h * f( y[i], t[i], *args )\n",
    "        k2 = h * f( y[i] + c21 * k1, t[i] + c20 * h, *args )\n",
    "        k3 = h * f( y[i] + c31 * k1 + c32 * k2, t[i] + c30 * h, *args )\n",
    "        # BUG: The ``-`` in the equation below should be a ``+``.\n",
    "        k4 = h * f( y[i] + c41 * k1 + c42 * k2 + c43 * k3, t[i] + c40 * h, *args )\n",
    "        k5 = h * f( y[i] + c51 * k1 + c52 * k2 + c53 * k3 + c54 * k4, \\\n",
    "                        t[i] + h, *args )\n",
    "        k6 = h * f( \\\n",
    "            y[i] + c61 * k1 + c62 * k2 + c63 * k3 + c64 * k4 + c65 * k5, \\\n",
    "            t[i] + c60 * h, *args )\n",
    "\n",
    "        y[i+1] = y[i] + a1 * k1 + a3 * k3 + a4 * k4 + a5 * k5\n",
    "        y5 = y[i] + b1 * k1 + b3 * k3 + b4 * k4 + b5 * k5 + b6 * k6\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we will develop a solver for the catalysis model. All, we need to do is define the right hand side of Eq. (\\ref{eq:kinetic_model}):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_catalysis(y, t, kappa):\n",
    "    rhs = np.zeros((6,))\n",
    "    rhs[0] = -kappa[0] * y[0]\n",
    "    rhs[1] = kappa[0] * y[0] - (kappa[1] + kappa[3] + kappa[4]) * y[1]\n",
    "    rhs[2] = kappa[1] * y[1] - kappa[2] * y[2]\n",
    "    rhs[3] = kappa[2] * y[2]\n",
    "    rhs[4] = kappa[3] * y[1]\n",
    "    rhs[5] = kappa[4] * y[1]\n",
    "    return rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to calibrate the reaction rate parameters of the model to the data. Because the parameters are two small, let us work with the transformed version:\n",
    "\n",
    "$$\n",
    "\\xi_i = \\log\\left(\\frac{k_i}{180}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the routine that returns $u(\\xi)$ for given values of $\\xi_1$, $\\xi_2$, $\\xi_3$, $\\xi_4$, $\\xi_5$ is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_model(xi1, xi2, xi3, xi4, xi5):\n",
    "    t = np.linspace(0, 180, 100)\n",
    "    kappa = np.exp([xi1, xi2, xi3, xi4, xi5]) / 180.\n",
    "    y = rk45(f_catalysis, (500., 0., 0., 0., 0., 0.), t, args=(kappa,))\n",
    "    return np.array([y[-1,0],y[-1,1],y[-1,3],y[-1,4],y[-1,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32.99401785  16.42517405 113.56266996 150.1936027  150.1936027 ]\n"
     ]
    }
   ],
   "source": [
    "y = comp_model(1,1,1,1,1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary routines for the computation of the integrated autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# LAOD REQUIRED MODULES\n",
    "# =====================\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "# import pymc as pm\n",
    "import pandas as pd\n",
    "from ipywidgets import interactive\n",
    "import scipy as scipy \n",
    "from scipy.stats import lognorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# COMPUTER MODEL FUNCTIONS\n",
    "# ========================\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(y, x, sigma):\n",
    "    likelihood = 0\n",
    "    mean = np.log(comp_model(x[0], x[1], x[2], x[3], x[4]))\n",
    "    sigma = sigma\n",
    "    for i in range(5):\n",
    "        likelihood += np.log(lognorm([sigma],loc=mean[i]).pdf(y[i]))\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-53.72986109]\n"
     ]
    }
   ],
   "source": [
    "y = [13.54, 6.11, 309.50, 27.26, 62.54]\n",
    "x0 = [1,1,1,1,1]\n",
    "like = log_likelihood(y,x0,1)\n",
    "print(like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01):\n",
    "    '''\n",
    "    Computes the EI at points X based on existing samples X_sample\n",
    "    and Y_sample using a Gaussian process surrogate model.\n",
    "    \n",
    "    Args:\n",
    "        X: Points at which EI shall be computed (m x d).\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "        xi: Exploitation-exploration trade-off parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Expected improvements at points X.\n",
    "    '''\n",
    "    mu, sigma = gpr.predict(X, return_std=True)\n",
    "    mu_sample = gpr.predict(X_sample)\n",
    "\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    \n",
    "    # Needed for noise-based model,\n",
    "    # otherwise use np.max(Y_sample).\n",
    "    # See also section 2.4 in [...]\n",
    "    mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def propose_location(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=25):\n",
    "    '''\n",
    "    Proposes the next sampling point by optimizing the acquisition function.\n",
    "    \n",
    "    Args:\n",
    "        acquisition: Acquisition function.\n",
    "        X_sample: Sample locations (n x d).\n",
    "        Y_sample: Sample values (n x 1).\n",
    "        gpr: A GaussianProcessRegressor fitted to samples.\n",
    "\n",
    "    Returns:\n",
    "        Location of the acquisition function maximum.\n",
    "    '''\n",
    "    dim = X_sample.shape[1]\n",
    "    min_val = 1\n",
    "    min_x = None\n",
    "    \n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr)\n",
    "    \n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    for x0 in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim)):\n",
    "        res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B')        \n",
    "        if res.fun < min_val:\n",
    "            min_val = res.fun[0]\n",
    "            min_x = res.x           \n",
    "            \n",
    "    return min_x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1 1 1 1 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-3d9501ddb01a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Update Gaussian process with existing samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mgpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Obtain next sampling point from the acquisition function (expected_improvement)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\haha\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_vector_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m             X, y = check_X_y(X, y, multi_output=True, y_numeric=True,\n\u001b[1;32m--> 191\u001b[1;33m                              ensure_2d=True, dtype=\"numeric\")\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             X, y = check_X_y(X, y, multi_output=True, y_numeric=True,\n",
      "\u001b[1;32m~\\.conda\\envs\\haha\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    756\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\.conda\\envs\\haha\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    554\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1 1 1 1 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "from bayesian_optimization_util import plot_approximation, plot_acquisition\n",
    "\n",
    "# Gaussian process with Mat??rn kernel as surrogate model\n",
    "noise = 1\n",
    "m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=m52, alpha=noise**2)\n",
    "\n",
    "# Initialize samples\n",
    "X_sample = x0\n",
    "Y_sample = like\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 10\n",
    "\n",
    "plt.figure(figsize=(12, n_iter * 3))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Update Gaussian process with existing samples\n",
    "    gpr.fit(X_sample, Y_sample)\n",
    "\n",
    "    # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "    X_next = propose_location(expected_improvement, X_sample, Y_sample, gpr, bounds)\n",
    "    \n",
    "    # Obtain next noisy sample from the objective function\n",
    "    Y_next = log_likelihood(X_next, noise)\n",
    "    \n",
    "    # Plot samples, surrogate function, noise-free objective and next sampling location\n",
    "    plt.subplot(n_iter, 2, 2 * i + 1)\n",
    "    plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next, show_legend=i==0)\n",
    "    plt.title(f'Iteration {i+1}')\n",
    "\n",
    "    plt.subplot(n_iter, 2, 2 * i + 2)\n",
    "    plot_acquisition(X, expected_improvement(X, X_sample, Y_sample, gpr), X_next, show_legend=i==0)\n",
    "    \n",
    "    # Add sample to previous samples\n",
    "    X_sample = np.vstack((X_sample, X_next))\n",
    "    Y_sample = np.vstack((Y_sample, Y_next))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
